---
title: "Milestone Report"
author: "Yanze Song"
date: "November 27, 2016"
output: html_document
---

The motivation for this project is to: 1. Demonstrate that you've downloaded the data and have successfully loaded it in.2. Create a basic report of summary statistics about the data sets.3. Report any interesting findings that you amassed so far.4. Get feedback on your plans for creating a prediction algorithm and Shiny app.

#1. Load and Understand the Data

```{r, warning=FALSE}
require(qdap)
require(ggplot2)
require(stringi)
require(quanteda)
require(tm)
require(RWeka)
#load data
setwd("/Users/yanze/github/capstone project/final/en_US/")
blogs_file <- "en_US.blogs.txt"
blogs_con <- file(blogs_file, "r")
blogs <- readLines(blogs_con, encoding = "UTF-8")

news_file <- "en_US.news.txt"
news_con <- file(news_file, "r")
news <- readLines(news_con, encoding = "UTF-8")

twitter_file <- "en_US.twitter.txt"
twitter_con <- file(twitter_file, "r")
twitter <- readLines(twitter_con, encoding = "UTF-8")

blog_size<- file.size("en_US.blogs.txt");
twitter_size<- file.size("en_US.twitter.txt");
news_size<- file.size("en_US.news.txt");
fileSize <- c(blog_size, twitter_size, news_size);

blogs_words<-sum(stri_count_words(blogs));
twitter_words<-sum(stri_count_words(twitter));
news_words<-sum(stri_count_words(news));
fileWords <- c(blogs_words, twitter_words, news_words);

blogs_gen<-stri_stats_general(blogs);
twitter_gen<-stri_stats_general(twitter);
news_gen<-stri_stats_general(news);
fileLines <- c(blogs_gen[1], twitter_gen[1], news_gen[1]);

fileNames <- c("blogs", "news", "twitter");
datInfo <- data.frame(cbind(fileNames, fileSize, fileWords, fileLines))
print(datInfo)
```

#2. Explore and analyse the text

```{r, message = FALSE, warning=FALSE}
require(qdap)
require(ggplot2)
require(stringi)
require(quanteda)
require(tm)
require(RWeka)
require(ngram)
#take 5% of the data as sample to analyse
set.seed(1234)
blogs_sample<- sample(blogs, length(blogs)*0.05, replace=F)
twitter_sample<- sample(twitter, length(twitter)*0.05, replace=F)
news_sample<- sample(news, length(news)*0.05, replace=F)
data_sample<- c(blogs_sample, twitter_sample, news_sample)

ALL_doc <-tm::Corpus(VectorSource(data_sample))
ALL_doc<-tm_map(ALL_doc,removeNumbers)
ALL_doc<-tm_map(ALL_doc,stripWhitespace)
ALL_doc<-tm_map(ALL_doc,content_transformer(tolower))
ALL_doc<-tm_map(ALL_doc,removePunctuation)
ALL_doc <- tm_map(ALL_doc, removeWords, stopwords("english"))  

writeLines(data_sample, "sampleData.txt")
rm(blogs_sample, twitter_sample, news_sample, data_sample)

words <- readLines('sampleData.txt', encoding = 'UTF-8')
myCorpus <- Corpus(VectorSource(words))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
#myCorpus <- tm_map(myCorpus, toSpace,"\"|/|@|\\|")
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
myCorpus <- tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus <- tm_map(myCorpus,removePunctuation)

data_corpus<-corpus(myCorpus)
tkn<- tokenize(data_corpus, removeNumbers = T, removePunct = T, removeSeparators = T)
tkn_dfm<- dfm(tkn, stem=T, ignoredFeatures = stopwords("english"))
plot(tkn_dfm, max.word=50, colors=brewer.pal(8, "Dark2"))

#defining the frequencies of one-grams, two-grams, and three-grams:
tkn_dfm_1<- dfm(data_corpus, ngrams=1, verbose= T, concatenator=" ", stopwords=T)
tkn_dfm_2<- dfm(data_corpus, ngrams=2, verbose= T, concatenator=" ", stopwords=T) 
tkn_dfm_3<- dfm(data_corpus, ngrams=3, verbose= T, concatenator=" ", stopwords=T) 

one_gram<-as.data.frame(as.matrix(docfreq(tkn_dfm_1)))
two_gram<-as.data.frame(as.matrix(docfreq(tkn_dfm_2)))
three_gram<-as.data.frame(as.matrix(docfreq(tkn_dfm_3)))


# Histogram of top 30 one-grams
one_gram_sorted<-sort(rowSums(one_gram), decreasing=T)
one_gram_table<-data.frame(Words=names(one_gram_sorted), Frequency=one_gram_sorted)
one_gram_plot<-ggplot(within(one_gram_table[1:30, ], Words<-factor(Words, levels=Words)), aes(Words, Frequency))+
  geom_bar(stat="identity", fill="green")+
  ggtitle("Top 30 one-grams")+ theme(axis.text.x=element_text(angle=45, hjust=1))
one_gram_plot
print(one_gram_table[1:50,])
# Histogram of top 30 two-grams
two_gram_sorted<-sort(rowSums(two_gram), decreasing=T)
two_gram_table<-data.frame(Words=names(two_gram_sorted), Frequency=two_gram_sorted)
two_gram_plot<-ggplot(within(two_gram_table[1:30, ], Words<-factor(Words, levels=Words)), aes(Words, Frequency))+
  geom_bar(stat="identity", fill="green")+
  ggtitle("Top 30 two-grams")+ theme(axis.text.x=element_text(angle=45, hjust=1))
two_gram_plot
# Histogram of top 30 three-grams
three_gram_sorted<-sort(rowSums(three_gram), decreasing=T)
three_gram_table<-data.frame(Words=names(three_gram_sorted), Frequency=three_gram_sorted)
three_gram_plot<-ggplot(within(three_gram_table[1:30, ], Words<-factor(Words, levels=Words)), aes(Words, Frequency))+
  geom_bar(stat="identity", fill="green")+
  ggtitle("Top 30 three-grams")+ theme(axis.text.x=element_text(angle=45, hjust=1))
three_gram_plot
```

#3.Project plan
For now, I can get some basic understanding from the input text data, I will keepling cleaning the data and get more insight of it. In addition, I plan to do more research on the pros and cons for one gram, two gram and three grams. 

Beside, I will start thinkng about the final app UI and server side design.
